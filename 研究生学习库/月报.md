# 四月月报

## 文献阅读

### Ada-LISTA: Learned Solvers Adaptive to Varying Models

[[002d Ada-LISTA Learned Solvers Adaptive to Varying Models]]

- 期刊：IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)
- 分区：一区/24.314
- 内容概述：提出了一种自适应学习求解器：Ada-LISTA。它接收成对的信号及其相应的字典作为输入，解决存在字典扰动时出现的问题。
- 数学模型：$$\begin{aligned}
& \mathbf{x}_{k+1}=\mathcal{S}_{\theta_{k+1}}\left(\left(\mathbf{I}-\gamma_{k+1} \mathbf{D}^T \mathbf{W}_1^T \mathbf{W}_1 \mathbf{D}\right) \mathbf{x}_k+\gamma_{k+1} \mathbf{D}^T \mathbf{W}_2^T \mathbf{y}\right)
\end{aligned}$$
- 网络架构：![[Ada-LISTA.png]]

### Learning to Optimize: A Primer and A Benchmark

[[005 Learning to Optimize： A Primer and A Benchmark]]

- 期刊：Journal of Machine Learning Research
- 分区：三区 / 5.177
- 内容概述：一篇关于学习优化（Learning to Optimize：L2O）的综述。学习优化 (L2O) 是一种利用机器学习开发优化方法的新兴方法，旨在减少手工的费力迭代。简而言之就是通过神经网络学习一种优化方法：$x_{t+1}=x_t-m(\mathbf{z}_t;\phi)$，实现参数的高效优化。L2O 分为基于 LSTM 的 Model-free L2O 以及基于算法展开的 Model-based L2O。文章最后给出了 L2O 算法的评价基准：Open-L2O。

### Building recurrent networks by unfolding iterative thresholding for sequential sparse recovery

[[002e Building recurrent networks by unfolding iterative thresholding for sequential sparse recovery]]

- 会议：IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
- 年份：2017
- 内容概述：文章提出了一种用于处理时序信号的稀疏恢复算法 SISTA，该算法接受一段时序相关的信号作为输入，并且考虑了信号在时间上的关联性。最后，基于 RNN 网络，将算法展开为 SISTA-RNN 网络。
- 网络架构： ![[SISTA Fig1.png]]



## 代码进展

### ISTA 算法的深度展开以实现 DOA 估计

1. 构建 Complex-LISTA 网络以实现 DOA 估计：
	1. 初步构建结构显示：在中低角度间隔实现很好的估计精度；但是一旦角度间隔过大，很大概率仅能估计单个甚至零个声源位置，并且精度显著下降
		1. 实验结果：![[Pasted image 20230409234357.png]]
		2. 可能原因：
			1. 网络的稀疏性太大。但是稀疏约束参数是通过学习得来的，所以可能原因有： 
				1. 网络标签设计有误；
				2. 网络损失函数设计不合理。
	2. 经过对训练数据集及其标签的检查发现：
		1. 训练数据集中的间隔角度未能完全包含测试集：
			1. 前210组数据体现模型的重构性能
			2. 其余数据用于评价模型对于间隔角的泛化性能
		2. 搜索重构功率谱峰值确定 DOA 角度时，设置了幅值阈值。而当测试间隔角大于训练集时，其重构的功率谱峰值幅值往往较低，可能会被忽略。
		3. 同时将训练标签设计为信号的真实功率谱
		4. 实验结果：使用 LISTA 网络展开方法对于不同间隔角的泛化性能较差。
			1. ![[Pasted image 20230505213543.png]]
			2. ![[Pasted image 20230505213546.png]]
			3. ![[Pasted image 20230505213550.png]]
			4. ![[Pasted image 20230505213603.png]]

# 五月月报

## 文献阅读

[[004a Optimization-Inspired Cross-Attention Transformer for Compressive Sensing]]
来源：  CVPR 2023
年份：2023
解决问题：
1. 往往以大量参数为代价来提高视觉质量
2. 在迭代过程中存在==特征信息丢失==的问题
提出方法：交叉注意力Transformer（==Cross-attention Transformer，OCT==）
	设计双交叉注意力（Dual Cross Attention，Dual-CA）模块
		1. InertiaSupplied Cross Attention （ISCA）块
		2. ProjectionGuided Cross Attention （PGCA）块
网络框架：
![[Pasted image 20230531134059.png]]

## 代码进展

### DCNN-DOA 复现问题：

#### 问题1: 对于低角度间隔下的耦合问题

论文原始代码结果：使用TensorFlow1运行吴流丽的卷积网络 DCNN-DOA 论文中给出的原始代码，仅修改了训练epoch数（3->300）和测试数据集的比例（0.99->0.2）。==运行结果和使用pytorch复现结果基本一致==，都是在==低角度间隔下出现耦合==现象。

![[Pasted image 20230516153658.png|500]]

原因：
1. 进一步检查数据集时发现，在提出 DCNN-DOA 网络的英文小论文中给出的测试集角度间隔为\[10.4, 15.2, 20.7, 25.9]，而代码中给出的测试集角度间隔为\[5.5, 13.5, 20.67, 50]（这与博士论文中的描述一致），而复现实验中出现耦合的部分正是角度间隔为 5.5 的部分。
2. 可能原因：作者在发布小论文之后，找到了解决耦合的办法，做进了毕业论文中，但是之前小论文中给的代码没有及时更新。


---

#### 问题2: 对于大概率无法训练的问题

方法：
1. 将训练数据的输入进行归一化处理，使信号的伪谱分布在0～1之间。由于伪谱数据中的虚部几乎为0（数量级为10e-15），仅仅对输入伪谱的实部做归一化处理
2. 同时对于标签，不再采用将真实 DOA 位置置“1”的方法，而是根据模型公式推导，将其设置为训练时信号的功率平方。

结果：没有再出现无法训练的问题。

$$x_{input}=\frac{x_{input}-\min(x_{input})}{\max(x_{input})-\min(x_{input})}$$

![[Pasted image 20230523222355.png|500]]

#### 修正后复现结果

##### Sample Index

数据集：角度间隔集 $[10.4,15.2,20.7,25.9]$

![[Pasted image 20230519225659.png|500]]
![[Pasted image 20230523153231.png|500]]

![[Pasted image 20230519225629.png|500]]

##### RMSE vs SNR

数据集：1000组 $[-10.5, 4.5]\pm N~(0,1)$

![[Pasted image 20230517172628.png|500]]

![[Pasted image 20230523223020.png|500]]

![[Pasted image 20230523223054.png|500]]

### Complex-LISTA问题：

将迭代目标 $\bf{\eta}$  在网络中始终设置为实数域，在软阈值函数时先对 $\bf{r}_k=\bf{W}_t \bf{\eta}^{k}+\bf{W}_e\bf{z}$ 求模，再应用软域值函数

问题：
1. 在重构功率谱时，波峰处往往带有一个较高的旁瓣，导致第二个声源的位置估计错误
2. 估计精度也较低（相对于DCNN）
3. 检查模型中的参数（软阈值函数的阈值）时发现，一共堆叠二十层，前6层的阈值几乎没有更新
4. 在测试不同噪声时，发现只有训练数据集对应的0dB处RMSE最低

![[Pasted image 20230523142549.png|500]]

![[Pasted image 20230523143128.png|500]]

![[Pasted image 20230523233534.png|500]]

## 解决方案

### DCNN-DOA

1. 确认网络的输入：$\tilde{\boldsymbol{\eta}}=\boldsymbol{W}^{\mathrm{H}}\hat{\boldsymbol{z}}$ ，其中 $\hat{\boldsymbol{z}}$ 是协方差向量，$\boldsymbol{W}^{\mathrm{H}}$ 是对应构造的字典矩阵。
2. 验证输入归一化和BN层对网络训练的重要性

### Complex-LISTA

1. 确认训练数据集的的SNR范围：文中没有提到向训练数据中添加噪声
2. 网络的稀疏性不足
3. 过拟合：L2-Weight Decay
4. 减少网络层数
5. Ada-LISTA


